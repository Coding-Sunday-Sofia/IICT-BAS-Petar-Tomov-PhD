\chapter{Предложение за алгоритми при прогнозиране и обучение на изкуствени невронни мрежи}

\section{Инкрементална апроксимация със синусоиди и тренд}

Тъй като времевите редове представляват измервания, извършени в строга последователност по отношение на оста на времето, получените точки на измерване могат да се подложат на анализ с инструментите за обработка на сигнали. През измерените точки или в близост до тях е възможно да се построят криви (пример е полиномът на Лагранж), описани с математически формули. При достигане на достатъчно близост до точките, надеждата е построените криви да имат поне частични свойства за формиране на прогноза, извън интервала на известните измервания, засягащ бъдещите моменти във времето. В общия случай става въпрос за търсене на подходящи коефициенти в математически формули. Търсенето на подходящите коефициенти е оптимизационна задача със значителна сложност, поради най-често използваните нелинейни компоненти във формулите. Колкото повече коефициенти са обект на оптимизация, толкова по-сложна става оптимизационната задача. По аналогия с трансформацията на Фурие, възможно е през времевия ред да се построи крива изградена от множество синус функции и уравнение на права (обозначава тренда). Теоретично е доказано, че през краен брой точки в двумерното пространство могат да се прекарат безкраен брой криви. Запазването на свойството за обобщение (прогноза) силно зависи от възможността броя синус функции да бъде по-възможност по-малък. 

При финансовите времеви редове е много характерно да има множество флуктуации (движение нагоре или надолу). Тази особеност прави финансовите времеви редове идеални кандидати за апроксимация, чрез ред от синус функции. Във финансовите времеви редове е изключителна рядкост да отсъства тренд. Най-често във финансовите времеви редове се откроява ясно различим тренд на нарастване или тренд на спадане. Трендът ефективно се описва с уравнение на права в двуизмерното пространство, като чрез методи като линейната регресия лесно може да се установят двата параметъра – наклон и срез. 

\begin{equation}  \label{equ001}
y(t) = B.t + C + A_1.sin(\omega_1.t+\varphi_1) + A_2.sin(\omega_2.t+\varphi_2) + ...
\end{equation}

Уравнението за апроксимация чрез тренд и синус функции (\ref{equ001}) съдържа множество коефициенти. Коефициентът $B$ задава наклона на правата. Коефициентът $C$ задава среза в уравнението на правата. Коефициентите $A_n$ задават амплитудата на синус функциите. Коефициентът $\omega_n$ задава ъгловата скорост. Коефициентът $\varphi_n$ задава фазовото отместване. Точният брой синус функции се определя от процедурата по инкрементално оптимизиране на коефициентите. Стремежът е този брой да бъде възможно най-малък, но и изчислените прогнозни стойности да бъдат възможно най-близки до реалните бъдещи стойности. 

Инкременталният процес по оптимизация започва с оптимизацията само на коефициентите в линейния компонент ($B$ и $C$). Постигнатите оптимални или субоптимални стойности за тренда служат като основа за разширяване на пространството на променливите, като се добавят три коефициента за първата синус функция ($A_1$, $\omega_1$ и $\varphi_1$). Тъй като оптимизаторът използван в LibreOffice Calc е стохастичен, базиран на еволюция на разликите и рояк от частици, процесът по оптимизация се прекратява когато постигнатото най-приближено решение не се подобрява в предварително зададен интервал от време. След изчерпването на възможностите за подобряване на решенията с една синус функция се добавя втора синус функция, която разширява пространството на променливите с още три ($A_2$, $\omega_2$ и $\varphi_2$). Добавянето на синус функции се съблюдава ръчно, тъй като всеки времеви ред се характеризира със своя собствена уникална форма. Важно е да не се добавят твърде много синус функции, защото това би довело до пренапасване на модела (overfitting) и до загуба на възможността за обобщаване (прогнозиране).

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{fig0007.png}
  \caption{Входни данни за цената на Bitcoin}
\label{fig0007}
\end{figure}

За демонстрация на описания метод се използват данни за стойността на дигиталната валута Bitcoin в щатски долари, на дневна база, за период от един месец (Фиг. \ref{fig0007}). Стохастичните оптимизационни алгоритми могат да започнат процеса за оптимизация от различни точки в пространството на променливите. Колкото по-благоприятни са началните точки, толкова по-големи са шансовете за достигане на глобално оптимално решение. В предложения пример оптимизацията започва от вектор съдържащ нули във всичките си компоненти. В колона $A$ е зададено времето под формата на числена стойност, според интерпретацията на LibreOffice Calc. В колона $B$ е стойността на отваряне (началото на интервала) за дигиталната валута в съответния ден. В колона $C$ е поместена пресметнатата прогнозна стойност. В колона $D$ е квадратния корен от втората степен на разликата между реалната стойност и прогнозираната стойност. Сумата от всички разлики е пресметната в клетка $M2$ (\ref{fig0008}).

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{fig0008.png}
  \caption{Настройки за оптимизация}
\label{fig0008}
\end{figure}

\section{Бърз прототип на LibreOffice Calc с еволюция на разликите и оптимизация с рояк от частици}

Процесът по търсенето на оптимални тегла в изкуствена невронна мрежа от тип трислоен перцептрон много нагледно може да бъде демонстриран с бърз прототип в софтуерния пакет LibreOffice Calc. За осъществяване на прототипирането моделът на изкуствената невронна мрежа бива разгърнат в двузимерната равнина от клетки на електронната таблица. Търсенето на оптимални стойности за теглата в мрежата се постига чрез вградения в LibreOffice Calc модул за оптимизация наречен Solver (Фиг. \ref{fig001}).

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{fig001.png}
  \caption{Модул за оптимизация в LibreOffice Calc}
\label{fig001}
\end{figure}

За нелинейна оптимизация модулът прилага алгоритмите за еволюция на разликите и оптимизация с рояк от частици. Двата алгоритъма се прилагат в хибридна комбинация, като с предварително дефинирана вероятност е определено колко често ще бъде активиран всеки от тях. Модулът се настройва за клетка, чиято оптимална стойност ще бъде търсена (максимум, минимум или конкретно число). Също така, задава се и регионът от клетки, които подлежат на промяна в процеса по оптимизация. Като клетка в която ще се търси минимум при бързото протитипиране се избира общата средно-квадратична грешка допусната от изкуствената невронна мрежа. Регионът от клетки за оптимизация съдържа теглата използевани в изкуствената невронна мрежа. 

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{fig002.png}
  \caption{Стойности на Bitcoin виртуалната валута}
\label{fig002}
\end{figure}

Като множество данни се използват стойностите на Bitcoin виртуалната валута (Фиг. \ref{fig002}), на дневна база, за няколко години назад. Моделът за прогнозиране се основава на нелинейна авторегресия. Това означава, че на входа на мрежата се подават мащабирани минали стойности от времевия ред, а на изхода на мрежата се очакват мащабирани прогнозни стойности. 

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{fig003.png}
  \caption{Параметри на трислойната изкуствена невронна мрежа}
\label{fig003}
\end{figure}

След подбора на времеви ред следва избор на параметрите с които ще бъде напревен модела на трислойната изкуствена невронна мрежа (Фиг. \ref{fig003}). За целите на линейното мащабиране първо се намират най-малката и най-голямата стойност в оригиналния времеви ред, чрез формули в LibreOffice Calc: $=MIN(A:A)$ и $=MAX(A:A)$. Тъй като приложената прагова функция е хиперболичен тангенс, диапазона за мащабирания времеви ред е избран от -0.5 до +0.5. Умишлено се избягва мащабиране до асимптотичните стойности от -1.0 до +1.0, тъй като такова мащабиране много би увеличило стойностите на междинните пресмятания, а и не би дало възможност да се прогнозират по-малки или по-големи стойности от вече известните в оригиналния времеви ред. Топологията на мрежата се избира експериментално, като изходния слой има размер, според това колко стойности в бъдещето е желателно да се предсказват. Размера на входния слой се определя експериментално. За размера на скрития слой има различни емпирични правила, като най-популярното е скритият слой да бъде половината от сумата на размерите имащи входния и изходния слой. Съществуват адаптивни алгоритми, които чрез проби и грешки да определят топологията на мрежата, но в това бързо прототипиране тези алгоритми не се прилагат. 

Общият брой стойности в оригиналния времеви ред се определят с формула в LibreOffice Calc: $=COUNT(A:A)$. Тъй като процеса по „разгръщането“ на модела е относително бавен, то се добавя клетка в която да се проследява напредъка от Python скрипта в „разгръщането“. LibreOffice Calc позволява изпълнението на макроси, като се поддържат няколко програмни езика. Програмният език Python е изключително популярен в сферата на машинното самообучение и дава изключително големи възможности за частична автоматизация в програмите на LibreOffice.

След мащабирането на оригиналния времеви ред се формират тренировъчните примери, като за вход се вземат стойности преди условния момент $t_0$, а за очакван изход стойности след условния момент $t_0$ (условно разделяне на минало и бъдеще).

\begin{lstlisting}[caption=Мащабиране на оригиналния времеви ред, language=Python, basicstyle=\tiny, label=listing001]
    ''' Scale input. '''
    for t in range(1, total_values + 1):
        sheet.getCellRangeByName("E" + str(t)).setValue(sheet.getCellRangeByName("$C$4").getValue() + 
        (sheet.getCellRangeByName("$C$5").getValue() - sheet.getCellRangeByName("$C$4").getValue()) * ((sheet.getCellRangeByName("A" + str(t)).getValue() - 
        sheet.getCellRangeByName("$C$1").getValue()) / (sheet.getCellRangeByName("$C$2").getValue() - sheet.getCellRangeByName("$C$1").getValue())))
\end{lstlisting}

В листинг \ref{listing001} се демонстрира линейното мащабиране, като за извършване на изчисленията се използват установените минимални и максимални стойности (Фиг. \ref{fig004}). 

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{fig004.png}
  \caption{Резултат от мащабирането на времевия ред}
\label{fig004}
\end{figure}

На всеки слой в изкуствената невронна мрежа се добавя по един допълнителен неврон (Листинг \ref{listing002}), постоянно емитиращ единична стойност (отместване или от английски език bias).

\begin{lstlisting}[caption=Неврони емитиращи постоянно единичен сигнал, language=Python, basicstyle=\tiny, label=listing002]
        ''' Setup biases. '''
        sheet.getCellRangeByName("G" + str(x)).setValue(1)
        sheet.getCellRangeByName("G" + str(x)).CellBackColor = (255 << 16 | 255 << 8 | 0)
        sheet.getCellRangeByName("H" + str(x)).setValue(1)
        sheet.getCellRangeByName("H" + str(x)).CellBackColor = (255 << 16 | 255 << 8 | 0)
        sheet.getCellRangeByName("I" + str(x)).setValue(1)
        sheet.getCellRangeByName("I" + str(x)).CellBackColor = (255 << 16 | 255 << 8 | 0)
        sheet.getCellRangeByName("J" + str(x)).setValue(1)
        sheet.getCellRangeByName("J" + str(x)).CellBackColor = (255 << 16 | 255 << 8 | 0)
\end{lstlisting}

Мащабираният времеви ред бива „разбит“ условно на „минали“ стойности и „бъдещи“ стойности. Миналите стойности стават входни сигнали за изкуствената невронна мрежа (Листинг \ref{listing003}).

\begin{lstlisting}[caption=Формиране на входния слой, language=Python, basicstyle=\tiny, label=listing003]
        ''' Input data loading. '''
        for i in range(1, input_size + 1):
            sheet.getCellRangeByName("G" + str(x + i)).setValue(sheet.getCellRangeByName("E" + str(t + i)).getValue())
            sheet.getCellRangeByName("G" + str(x + i)).CellBackColor = (255 << 16 | 0 << 8 | 0)
\end{lstlisting}

По аналогичен начин, будещите стойности се зареждат като очакван изход от изкуствената невронна мрежа (Листинг \ref{listing004}).

\begin{lstlisting}[caption=Очакван изход от мрежата, language=Python, basicstyle=\tiny, label=listing004]
        ''' Expected data loading. '''
        for e in range(1, output_size + 1):
            sheet.getCellRangeByName("J" + str(x + e)).setValue(sheet.getCellRangeByName("E" + str(t + e + input_size)).getValue())
            sheet.getCellRangeByName("J" + str(x + e)).CellBackColor = (0 << 16 | 127 << 8 | 0)
\end{lstlisting}

Стойностите във възлите на скрития слой са резултат от пресмятане на входните сигнали и текущите стойности на теглата в мрежата (Листинг \ref{listing005}). 

\begin{lstlisting}[caption=Стойности на скрития слой при правия пас, language=Python, basicstyle=\tiny, label=listing005]
        ''' Setup hidden layer. '''
        wih = 1
        for h in range(1, hidden_size + 1):
            sum = ""
            for i in range(0, input_size + 1):
                sum = sum + "G" + str(x + i) + "*Q" + str(wih)
                wih = wih + 1
                if i < input_size:
                    sum = sum + " + "
            sheet.getCellRangeByName("H" + str(x + h)).setFormula("=TANH( " + sum + " )")
            sheet.getCellRangeByName("H" + str(x + h)).CellBackColor = (0 << 16 | 0 << 8 | 255)
\end{lstlisting}

На свой ред, стойностите в изходния слой са резултат от пресмятане на сигналите в скрития слой и текущите стойности на теглата в мрежата (Листинг \ref{listing006}).

\begin{lstlisting}[caption=Стойности на изходния слой при правия пас, language=Python, basicstyle=\tiny, label=listing006]
        ''' Setup output layer. '''
        who = 1
        for o in range(1, output_size + 1):
            sum = ""
            for h in range(0, hidden_size + 1):
                sum = sum + "H" + str(x + h) + "*S" + str(who)
                who = who + 1
                if h < hidden_size:
                    sum = sum + " + "
            sheet.getCellRangeByName("I" + str(x + o)).setFormula("=TANH( " + sum + " )")
            sheet.getCellRangeByName("I" + str(x + o)).CellBackColor = (0 << 16 | 255 << 8 | 0)
\end{lstlisting}

Стойностите в изходния слой, спрямо очакваните стойности дават грешката, която изкуствената невронна мрежа допуска за конкретния тренировъчен пример (Листинг \ref{listing007}).

\begin{lstlisting}[caption=Стойност на грешката допусната от мрежата за конкретния пример, language=Python, basicstyle=\tiny, label=listing007]
        ''' Network output error. '''
        for r in range(1, output_size + 1):
            sheet.getCellRangeByName("K" + str(x + r)).setFormula("= (J" + str(x + r) + "-I" + str(x + r) + ") * (J" + str(x + r) + "-I" + str(x + r) + ")")
            sheet.getCellRangeByName("K" + str(x + r)).CellBackColor = (0 << 16 | 255 << 8 | 255)
\end{lstlisting}

Така описаното разполагане по клетките на листа в електронната таблица се повтаря многократно, така че да се появи фрагмент за всеки тренировъчен пример. Фрагментът съдържа входен слой, скрит слой, изходен слой и очаквани на изхода стойности (Фиг. \ref{fig005}).

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{fig005.png}
  \caption{Фрагменти за тренировъчните примери}
\label{fig005}
\end{figure}

Общата грешка, допусната от мрежата при всички тренировъчни примери е на базата на средно-квадратична грешка (Листинг \ref{listing008}).

\begin{lstlisting}[caption=Обща средно-квадратична грешка на мрежата, language=Python, basicstyle=\tiny, label=listing008]
    ''' Network total error. '''
    sheet.getCellRangeByName("M1").setFormula("= SQRT( SUM(K:K) / COUNT(K:K) )")
    sheet.getCellRangeByName("M1").CellBackColor = 0
\end{lstlisting}

Два региона клетки в листа на електронната таблица се определят за стойностите на теглата в мрежата, както и обратно мащабиране към оригиналните стойности (Листинг \ref{listing009}).

\begin{lstlisting}[caption=Определяне на региони за теглата на мрежата, language=Python, basicstyle=\tiny, label=listing009]
    ''' Setup hidden layer weights. '''
    wih = 1
    for h in range(2, hidden_size + 2):
        for i in range(1, input_size + 2):
            sheet.getCellRangeByName("Q" + str(wih)).CellBackColor = (255 << 16 | 0 << 8 | 255)
            wih = wih + 1
        
    ''' Setup output layer weights. '''
    who = 1
    for o in range(2, output_size + 2):
        for h in range(1, hidden_size + 2):
            sheet.getCellRangeByName("S" + str(who)).CellBackColor = (255 << 16 | 0 << 8 | 255)
            who = who + 1

        sheet.getCellRangeByName("U" + str(o)).setFormula("=$C$1 + ($C$2 - $C$1) * ((T" + str(o) + " - $C$4) / ($C$5 - $C$4))")
        sheet.getCellRangeByName("U" + str(o)).CellBackColor = (0 << 16 | 127 << 8 | 0)
\end{lstlisting}

За да не се блокира графичния потребителски интерфейс, „разгръщането“ на модела на изкуствената невронна мрежа се извършва в отделна нишка (Листинг \ref{listing010}).

\begin{lstlisting}[caption=Изпълнение с отделна нишка, language=Python, basicstyle=\tiny, label=listing010]
def BuildAnnModel():
    thread = Thread(target=ThreadWorker, args=(XSCRIPTCONTEXT.getDesktop(),))
    thread.start()
\end{lstlisting}

Търсенето на оптимални тегла става чрез избор на клетката за която ще се търси минимална стойност (общата допусната грешка от мрежата), както и с избор на клетките, които Solver модулът може да променя, така че да удовлетвори търсенето на минимума (стойностите на теглата в мрежата). От диалоговата кутия за опции на Solver модула могат да се изберат различни параметри за алгоритмите еволюция на разликите и оптимизация с рояк на частици (Фиг. \ref{fig006}).

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{fig006.png}
  \caption{Избор на клетки за оптимизация и параметри на оптимизиращите алгоритми}
\label{fig006}
\end{figure}

