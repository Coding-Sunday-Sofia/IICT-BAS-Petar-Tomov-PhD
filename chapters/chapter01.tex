\chapter{Обзор на методите за прогнозиране с машинно самообучение}

\section{Прогнозиране на времеви редове}

Времевите редове са последователност от измервания, която следва строг ред във времето \cite{Oliveira-01}. Измерванията най-често са на равни интервали, но не винаги това е възможно или рационално \cite{Shen-01}. Изчислението на прогноза, базираща се на миналите стойности във времевия ред е своеобразна екстраполация \cite{Khashei-03}. При времеви редове, където нови стойности се отчитат твърде бързо във времето (примерно, едноминутен интервал на FOREX пазара) е нужно инструментът за прогнозиране да се обучава динамично, паралелно с режима за употреба \cite{Wagner-01}. В някои редки случаи информацията за времевия ред трябва да бъде извлечена от съдържанието на неструктурирани документи \cite{Wang-01}. В началото на 70-те години на XX век \cite{Gooijer-01} се предлагат линейни модели за прогнозиране на времеви редове – авторегресионен (Autoregressive) и плъзгащи средни (Moving Averages) \cite{Tealab-01}. При авторегресионния модел се възприема, че прогнозната стойност е линейна комбинация от стойностите в миналото. При плъзгащите средни прогнозната стойност е функция на случайни смущения - смущения, които са повлияли на времевия ред. Между двата модела е възможна комбинация под формата на авторегресионна интегрирана плъзгаща средна (Auto-Regressive Integrated Moving Average) \cite{Khashei-01}. При определянето на параметрите за авторегресията и интегрираната плъзгаща средна успешно могат да се приложат еволюционни алгоритми \cite{Cortez-01}. Линейните модели се оказват неприложими за множество времеви редове от реалната практика \cite{Bontempi-01}. Изкуствените неверонни мрежи се оказват един от подходите да се премине от линейност \cite{Zhang-03} към нелинейност в моделите. Трудностите при изкуствените невронни мрежи са свързани с по-големия брой параметри, които са трудни за определяне, като степен за обучение (learning rate) при обратно разпространение на грешката или размер на скрития слой \cite{Tang-01}. В някои разработки се правят опити да се автоматизира определянето на параметрите \cite{Yan-01}. Липсата на систематизиран подход за изграждане на изкуствени невронни мрежи \cite{Qi-01} е пряко свързан с определянето на параметрите. Линейност и нелинейност могат да се съчетаят в хибридни реализации, базирани на плъзгащи средни и изкуствени невронни мрежи \cite{Zhang-01}. Недостатък при изкуствените неверонни мрежи е нуждата от по-голям брой тренировъчни примери \cite{Lachtermacher-01}. Освен броя на тренировъчните примери, изключително съществен параметър се явява размерът на входния слой, който определя прозореца от стойности в миналия периода от миналото \cite{Chen-01}. С добавяне на обратни връзки в изкуствените невронни мрежи се постига ефект на кратковременна памет. Ефектът на кратковременната памет в изкуствените невронни мрежи често води до подобряване на получените прогнози \cite{Cao-02}. Ускорение в процеса по обучението на изкуствени невронни мрежи може да се получи чрез използването на вероятностни невронни мрежи (Probabilistic Neural Networks) \cite{Khashei-02}. Комбинацията от линейни уравнения за постигането на разграничение при нелинейни класове се постига и чрез машина с поддържащи вектори (Support Vector Machines) \cite{Kyoung-jae-01}. Машините с поддържащи вектори се явяват подобрение на изкуствените невронни мрежи обучавани по правилото за обратно разпространение на грешката. Времето за обучение се намалява, докато достоверността на прогнозите се увеличава \cite{Tay-01}. Ефективността на машините с поддържащи вектори може да се подобри, когато се използват алгоритми за адаптиране на параметрите \cite{Cao-01}. При времеви редове с ясно изразен тренд и сезонност \cite{Zhang-04}, премахването им може да подобри възможностите за генериране на прогноза \cite{Zhang-02}, включително и в комбинация с изкуствени неверонни мрежи \cite{Jain-01}. От оригиналния времеви ред се изваждат стойностите на тренда и подбрани циклични функции \cite{Nelson-01} или уейвлети \cite{Joo-01}. При слабо изразена сезонност, по-удачно може да се окаже сезонността да не се премахва \cite{Hamzacebi-01}. Възможност за генериране на прогнози дават и алгоритмите за разпознаване на образи (Patterns Recognition). Времевият ред може да бъде изследван за наличието на конкретни шаблони/образи и прогнозата да се разпознава на появата на конкретен шаблон \cite{Singh-01}. При много зашумени времеви редове е удачно да се приложи предварителна обработка на данните, така че шумът да се премахне или редуцира \cite{Lu-01}.

\section{Обучение на изкуствени невронни мрежи}

Най-разпространеният вид изкуствени неверонни мрежи е многослойният перцептрон. Структурата им е под формата на насочен тегловен граф, който може да има различни конфигурации \cite{da-Silva-01}. Организацията е на слоеве \cite{Jain-02} (най-често три - входен, скрит и изходен \cite{Pradhan-01}), като най-популярната практика е всички възли между два съседни слоя да са пълно свързани по между си. Пълната свързаност между слоевете понякога се нарушава с цел по-ефективно обучение \cite{Mocanu-01}. Този тип мрежи се използват за решаването на задачи в които са налични тренировъчни примери. Такъв тип обучение е известно като обучение с учител. Целта на изкуствената невронна мрежа е по входно-изходните двойки (тренировъчни примери) да формира функционална зависимост \cite{Kattan-01}. Този механизъм на работа прави многослойните перцептрони идеални за решаване на задачи за класификация \cite{Kubat-01} или прогнозиране \cite{Basheer-01}. Прогнозирането във финансовия свят е задача с голяма сложност при силна нелинейност, зависеща много и различни фактори \cite{Bing-01}. Процесът на самото обучение се състои в намирането на такива стойности за теглата в графа, че изкуствената невронна мрежа максимално добре да онагледява зависимостта между входните данни и изходните данни, без това да доведе до пренапасване (overfitting) \cite{Bilbao-01}. Задачата е оптимизационна и е дефинирана в многомерни нелинейни пространства \cite{Kingston-01}. Най-често началните стойности на теглата се избират случайно в близка околност около нулата, но е възможно първоначалните стойности да се определят с някаква форма на евристика, примерно генетични алгоритми \cite{Chandwani-01}. Теглата на мрежата най-често участват в линейна комбинация с входните сигнали. Получената сума се подлага на нормиране, чрез прилагане на прагова функция (функция на активация) \cite{Ertugrul-01}. Предварителна обработка на данните, преди да бъдат подадени на входа на изкуствената невронна мрежа, често подобряват процеса на обучението \cite{Nawi-01}. Липсващи стойности в множеството от входни данни оказват влияние при процеса на обучението на изкуствената невронна мрежа \cite{Viharos-01}. Организация от три слоя е една от най-често използваните в съчетание с точен числен метод за обучение наречен обратно разпространение на грешката \cite{Pomerleau-01}. Алгоритъмът с обратно разпространение на грешката дава възможности да бъде подобряван по различни начини \cite{Kollias-01}, което най-много се налага заради склонността да попада в локални минимуми \cite{Chen-02}. Освен точни числени методи, при обучението на изкуствени неверонни мрежи приложение намират и стохастичните еволюционни методи \cite{Slowik-01}, които често са вдъхновени от природни феномени \cite{Cui-01}. Точните числени методи превъзхождат евристичните по ефективност на обучението \cite{Piotrowski-01}, но евристичните методи дават повече възможности за избягване на локални оптимуми. Също така, градиентните точни числени методи изискват диференцируеми функции \cite{Karaboga-01}, което спомага в търсенето на глобални оптимуми \cite{Roy-01}. Популационните евристични методи имат много висока степен за паралелна обработка и са изключително подходящи при паралелни изчисления \cite{Ding-01} или разпределени изчисления. Добавянето на случаен шум (примерно нормално разпределен \cite{Zur-01}) може значително да подобри процеса по обучението на многослойни изкуствени невронни мрежи \cite{Sietsma-01}. Случайното изключване на различни възли от скрития слой също може да доведе до подобрено обучение \cite{Sequin-01}. Освен изключването на неврони, възможно е обучението да започва с по-голяма по размер мрежа, а с времето размера да се намалява (алгоритъм за подрязване) \cite{Karnin-01}. Противоположно на намаляването е подхода за избор на мрежова топология чрез нарастване \cite{Yao-01}. В някои ситуации нарастването на размера на изкуствената невронна мрежа води до експоненциално нарастване на нужните за обучението изчислителни ресурси \cite{Wilamowski-01}. Ранно спиране на обучението също може да подобри обобщаващите свойства на изкуствената невронна мрежа \cite{Coulibaly-01}.

\section{Евристични оптимизационни алгоритми}

Двете основни направления в глобалните оптимизационни алгоритми са точните числени методи и евристичните. При точните числени методи най-често се използва пресмятането на градиент и следването му. Те често са неприложими за нелинейни проблеми \cite{Koziel-01} или в зашумени входни данни \cite{Beyer-02}. При евристичните методи за глобална оптимизация (еднокритериална или многокритериална \cite{Cheng-01}) се залага на някаква евристика (интуитивно очакване) за намиране на решения по-близки до глобалните оптимуми (не се гарантира достигането на глобалния оптимум \cite{Coello-02}), като приложението е в множество области \cite{Slowik-02}, в които точните числени методи не биха дали разумен резултат в приемливо време. Възможностите в евристиките са толкова големи, че евристични алгоритми може да се ползват за проектиране на евристични алгоритми \cite{Diosan-01}. Целта при евристичните методи е да се постигне глобална сходимост \cite{He-02} в търсене на оптимално решение \cite{Beyer-01}, което понякога се постига и с комбинация от евристики \cite{Grosan-01}, като дори в такава конфигурация евристиките не винаги са достатъчно ефективни \cite{He-01}. Евристичните методи много често се реализират с помощта на генерирани случайни числа (така наречената генериране-тестване стратегия \cite{Yao-02}), генератори на псевдо-случайни числа \cite{Kazimipour-01} (чувствителни по отношение на началната инициализация \cite{Eiben-02}) или хаотични последователности \cite{Caponetto-01}. Докато за определени части от евристиката има теоретична обосновка, то в повечето случаи се използват и параметри за които няма теоретична обосновка, а се избират стойности по подразбиране (примерно, вероятността за кръстосване и вероятността за мутация при генетичните алгоритми) \cite{Eiben-01}. Един от подходите за избор на стойности за параметрите е преди стартирането на изчисленията, а другия подход е адаптивна промяна на стойностите, по време на самото изпълнение. В някои разработки евристични оптимизационни методи се използват за настройка на параметрите при евристична глобална оптимизация. Чрез локално търсене може да се прави фина настройка на параметрите, в самия процес по оптимизация \cite{Karafotias-01}, както и чрез статистически анализ \cite{Francois-01}. Когато евристичните методи се базират на множество от кандидат решения (популация), то те спадат към групата на популационните евристични алгоритми \cite{Whitley-01}. Популацията може да е организирана в еволюционен процес (примерно генетични алгоритми или еволюция на разликите) или в поведение на индивидите (примерно колония на мравките и рояк от частици) \cite{El-Abd-01}. Най-честият подход за попълване на популацията в началото е на случаен принцип, но са възможни и решения с предварително подбрани индивиди \cite{Coello-01}. Всеки индивид в популацията представлява вектор от пространството на променливите (цели или дробни числа \cite{Kachitvichyanukul-01}). Този вектор може да бъде с фиксирана или с плаваща дължина \cite{Ryerkerk-01}. Генетичното разнообразие в популацията може да бъде ключов фактор за ефективен оптимизационен процес \cite{Ursem-01}. При случаите в които наличната популация не съдържа достатъчно разнообразни решения, така че някои решения в пространството на търсенето стават недостижими, може да се приложи схема за въвеждане на изцяло новосъздадена популация \cite{Wegener-01}. Наличието на множество кандидат решения и появата на нови кандидат решения в процеса на оптимизация, често води до забавяне в оценката на решенията, най-вече когато целевата функция се изчислява бавно. При бавни за пресмятане целеви функции \cite{Naudts-01} е разумно да се търсят някакви начини за ускорена оценка на кандидат решенията \cite{Salami-01}. В процеса на евристична оптимизация може да се приложи изследване на пространството и апроксимация с експериментално определено вероятностно разпределение да ускори намирането на оптимално или субоптимално решение \cite{Kern-01}. Едно от най-големите предимства на популационните алгоритми е възможността да се изпълняват паралелно или в разпределена среда \cite{Vikhar-01}. Дори, когато изпълнението се извършва с програмни средства за паралелна обработка, но не еднопроцесорна машина някои популационни алгоритми дават по-добри резултати, спрямо последователната версия на кода \cite{Alba-01}. При оптимизационни проблеми с наложени ограничения (особено нелинейни ограничения) допълнителни мерки трябва да се вземат при генерирането на недопустими решения и разрешаването на такива решения да бъдат част от популацията \cite{Lagaros-01}.

\section{Изчисления в разпределена среда}

Една значителна част от съвременните алгоритми са изцяло линейни. Това означава, че всяка следваща инструкция зависи от резултатите пресметнати в предходната инструкция. Тази особеност не позволява линейните алгоритми да бъдат изпълнявани на повече от едно аритметично-логическо устройство. Единственият начин за ускоряване на пресмятанията при линейните алгоритми е чрез скъсяване на времето за което се изпълняват различните инструкции. Друга, доста по-малка част от съвременните алгоритми позволяват различни групи от инструкции да се пресмятат едновременно. Един от най-популярните примери е бързото сортиране (QuickSort). Алгоритъмът при бързото сортиране е свързан с първоначално разделяне на масива на две части. След първия пас при възходящо сортиране, лявата част съдържа по-малките елементи, а дясната част по-големите елементи. На свой ред, двете части на масива се възприемат като два разбъркани независими масива. Всяка от двете половини може да се подаде на различно ядро или процесор за пресмятане. От двете половини се получават четири нови под масива, след това осем под масива и т.н. От една страна, бързото сортиране има рекурсивна природа, но от друга страна то е идеален кандидат за паралелни пресмятания. Раздробяването на половини може да продължи докато се натоварят всичките налични ядра/процесори. Те заедно работят над обща памет, тъй като няма припокриване между под масивите. Групата на паралелните алгоритми са подходящи за изпълнение на многопроцесорни системи, които споделят обща памет. Стъпка напред в децентрализираното пресмятане е когато различните процесори са в отделни изчислителни машини, които разполагат със своя собствена памет. Този вид пресмятания се организират в клъстъри. Често клъстърите са съставени от еднотипни изчислителни машини, но това не е задължително условие. Когато изчислителните машини териториално се намират на различни места и контрола върху тях се осъществява от различни хората, изчисленията са в разпределена среда. Някои ниско-бюджетни проекти не могат да си позволят наемането на голям брой изчислителна техника и в такава ситуация се създават дарени изчисления в разпределена среда (Donate/Volunteer Distributed Computing). При дарените разпределени изчисления, хора ентусиасти преотстъпват изчислителната си техника безвъзмездно, така с нея да се извършват изчисления в периодите на занижена употреба. Много често такъв вид проекти се реализират под формата на скринсейвъри (Screensavers). Скринсейвърите се появяват заедно с катодно лъчевите тръби при мониторите и задачата им е била да съхраняват покритието на монитора в периоди в които потребителят не използва компютърната си система. Активацията на скринсейвърът е ясен индикатор, че компютърната система преминава в състояние на намалена употреба. С масовото навлизане на мобилните устройства през последните петнадесет години, бяха открити нови възможности за дарени изчисления в разпределена среда, които се извършват на умни телефони или таблети, докато те са в режим на занижена употреба. Основно предимство на мобилните устройства е, че те работят в непрекъснат режим от 24/7, но пък това е свързано с недостатъка, че изчислителната мощност е значително по-ниска в сравнение със съвременна настолна компютърна система. 

Възможността изкуствените невронни мрежи да се обучават с еволюционни алгоритми позволява ефективната реализация на такова обучение в разпределена среда \cite{Pandey-01}. Освен оптимизация на теглата в мрежата е възможна и оптимизация на структурата й \cite{Desell-01}. Характерно за разпределените изчисления е високата консумация на електрическа енергия \cite{Foo-01}. Обучението на изкуствени невронни мрежи с еволюционни алгоритми може да се реализира и в разпределена среда със специализиран хардуер \cite{Epitropakis-01}.

