\chapter{Обзор на методите за прогнозиране с машинно самообучение}

\section{Прогнозиране на времеви редове}

Времевите редове са последователност от измервания, която следва строг ред във времето \cite{Oliveira-01}. Измерванията най-често са на равни интервали, но не винаги това е възможно или рационално \cite{Shen-01}. Изчислението на прогноза, базираща се на миналите стойности във времевия ред е своеобразна екстраполация \cite{Khashei-03}. При времеви редове, където нови стойности се отчитат твърде бързо във времето (примерно, едноминутен интервал на FOREX пазара) е нужно инструментът за прогнозиране да се обучава динамично, паралелно с режима за употреба \cite{Wagner-01}. В някои редки случаи информацията за времевия ред трябва да бъде извлечена от съдържанието на неструктурирани документи \cite{Wang-01}. В началото на 70-те години на XX век \cite{Gooijer-01} се предлагат линейни модели за прогнозиране на времеви редове – авторегресионен (Autoregressive) и плъзгащи средни (Moving Averages) \cite{Tealab-01}. При авторегресионния модел се възприема, че прогнозната стойност е линейна комбинация от стойностите в миналото. При плъзгащите средни прогнозната стойност е функция на случайни смущения - смущения, които са повлияли на времевия ред. Между двата модела е възможна комбинация под формата на авторегресионна интегрирана плъзгаща средна (Auto-Regressive Integrated Moving Average) \cite{Khashei-01}. При определянето на параметрите за авторегресията и интегрираната плъзгаща средна успешно могат да се приложат еволюционни алгоритми \cite{Cortez-01}. Линейните модели се оказват неприложими за множество времеви редове от реалната практика \cite{Bontempi-01}. Изкуствените неверонни мрежи се оказват един от подходите да се премине от линейност \cite{Zhang-03} към нелинейност в моделите. Трудностите при изкуствените невронни мрежи са свързани с по-големия брой параметри, които са трудни за определяне, като степен за обучение (learning rate) при обратно разпространение на грешката или размер на скрития слой \cite{Tang-01}. В някои разработки се правят опити да се автоматизира определянето на параметрите \cite{Yan-01}. Липсата на систематизиран подход за изграждане на изкуствени невронни мрежи \cite{Qi-01} е пряко свързан с определянето на параметрите. Линейност и нелинейност могат да се съчетаят в хибридни реализации, базирани на плъзгащи средни и изкуствени невронни мрежи \cite{Zhang-01}. Недостатък при изкуствените неверонни мрежи е нуждата от по-голям брой тренировъчни примери \cite{Lachtermacher-01}. Освен броя на тренировъчните примери, изключително съществен параметър се явява размерът на входния слой, който определя прозореца от стойности в миналия периода от миналото \cite{Chen-01}. С добавяне на обратни връзки в изкуствените невронни мрежи се постига ефект на кратковременна памет. Ефектът на кратковременната памет в изкуствените невронни мрежи често води до подобряване на получените прогнози \cite{Cao-02}. Ускорение в процеса по обучението на изкуствени невронни мрежи може да се получи чрез използването на вероятностни невронни мрежи (Probabilistic Neural Networks) \cite{Khashei-02}. Комбинацията от линейни уравнения за постигането на разграничение при нелинейни класове се постига и чрез машина с поддържащи вектори (Support Vector Machines) \cite{Kyoung-jae-01}. Машините с поддържащи вектори се явяват подобрение на изкуствените невронни мрежи обучавани по правилото за обратно разпространение на грешката. Времето за обучение се намалява, докато достоверността на прогнозите се увеличава \cite{Tay-01}. Ефективността на машините с поддържащи вектори може да се подобри, когато се използват алгоритми за адаптиране на параметрите \cite{Cao-01}. При времеви редове с ясно изразен тренд и сезонност \cite{Zhang-04}, премахването им може да подобри възможностите за генериране на прогноза \cite{Zhang-02}, включително и в комбинация с изкуствени неверонни мрежи \cite{Jain-01}. От оригиналния времеви ред се изваждат стойностите на тренда и подбрани циклични функции \cite{Nelson-01} или уейвлети \cite{Joo-01}. При слабо изразена сезонност, по-удачно може да се окаже сезонността да не се премахва \cite{Hamzacebi-01}. Възможност за генериране на прогнози дават и алгоритмите за разпознаване на образи (Patterns Recognition). Времевият ред може да бъде изследван за наличието на конкретни шаблони/образи и прогнозата да се разпознава на появата на конкретен шаблон \cite{Singh-01}. При много зашумени времеви редове е удачно да се приложи предварителна обработка на данните, така че шумът да се премахне или редуцира \cite{Lu-01}.

\section{Обучение на изкуствени невронни мрежи}

Най-разпространеният вид изкуствени неверонни мрежи е многослойният перцептрон. Структурата им е под формата на насочен тегловен граф, който може да има различни конфигурации \cite{da-Silva-01}. Организацията е на слоеве \cite{Jain-02}, като най-често всички възли между два съседни слоя са пълно свързани по между си. Този тип мрежи се използват за решаването на задачи в които са налични тренировъчни примери. Такъв тип обучение е известно като обучение с учител. Целта на изкуствената невронна мрежа е по входно-изходните двойки тренировъчни примери да формира функционална зависимост. Този механизъм на работа прави многослойните перцептрони идеални за решаване на задачи за класификация \cite{Kubat-01} или прогнозиране \cite{Basheer-01}. Процесът на самото обучение се състои в намирането на такива стойности за теглата в графа, че изкуствената невронна мрежа максимално добре да онагледява зависимостта между дходните данни и изходните данни. Задачата е оптимизационна и е дефинирана в многомерни нелинейни пространства \cite{Kingston-01}. Предварителна обработка на данните, преди да бъдат подадени на входа на изкуствената невронна мрежа, често подобряват процеса на обучението \cite{Nawi-01}. Организация от три слоя е една от най-често използваните в съчетание с точен числен метод за обучение наречен обратно разпространение на грешката \cite{Pomerleau-01}. Алгоритъмът с обратно разпространение на грешката дава възможности да бъде подобряван по различни начини \cite{Kollias-01}. Освен точни числени методи, при обучението на изкуствени неверонни мрежи приложение намират и стохастичните еволюционни методи \cite{Slowik-01}, които често са вдъхновени от природни феномени \cite{Cui-01}. Точните числени методи превъзхождат евристичните по ефективност на обучението \cite{Piotrowski-01}, но евристичните методи дават повече възможности за избягване на локални оптимуми. Популационните евристични методи имат много висока степен за паралелна обработка и са изключително подходящи при паралелни изчисления \cite{Ding-01} или разпределени изчисления. Добавянето на случаен шум може значително да подобри процеса по обучението на многослойни изкуствени невронни мрежи \cite{Sietsma-01}. Случайното изключване на различни възли от скрития слой също може да доведе до подобрено обучение \cite{Sequin-01}. Освен изключването на неврони, възможно е обучението да започва с по-голяма по размер мрежа, а с времето размера да се намалява (алгоритъм за подрязване) \cite{Karnin-01}. Ранно спиране на обучението също може да подобри обобщаващите свойства на изкуствената невронна мрежа \cite{Coulibaly-01}.

\section{Еволюционни оптимизационни алгоритми}

\section{Изчисления в разпределена среда}


