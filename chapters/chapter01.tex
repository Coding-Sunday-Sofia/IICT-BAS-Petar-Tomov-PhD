\chapter{Обзор на методите за прогнозиране с машинно самообучение}

\section{Прогнозиране на времеви редове}

Времевите редове са последователност от измервания, която следва строг ред във времето \cite{Oliveira-01}. Измерванията най-често са на равни интервали, но не винаги това е възможно или рационално \cite{Shen-01}. Изчислението на прогноза, базираща се на миналите стойности във времевия ред е своеобразна екстраполация \cite{Khashei-03}. При времеви редове, където нови стойности се отчитат твърде бързо във времето (примерно, едноминутен интервал на FOREX пазара) е нужно инструментът за прогнозиране да се обучава динамично, паралелно с режима за употреба \cite{Wagner-01}. В някои редки случаи информацията за времевия ред трябва да бъде извлечена от съдържанието на неструктурирани документи \cite{Wang-01}. В началото на 70-те години на XX век \cite{Gooijer-01} се предлагат линейни модели за прогнозиране на времеви редове – авторегресионен (Autoregressive) и плъзгащи средни (Moving Averages) \cite{Tealab-01}. При авторегресионния модел се възприема, че прогнозната стойност е линейна комбинация от стойностите в миналото. При плъзгащите средни прогнозната стойност е функция на случайни смущения - смущения, които са повлияли на времевия ред. Между двата модела е възможна комбинация под формата на авторегресионна интегрирана плъзгаща средна (Auto-Regressive Integrated Moving Average) \cite{Khashei-01}. При определянето на параметрите за авторегресията и интегрираната плъзгаща средна успешно могат да се приложат еволюционни алгоритми \cite{Cortez-01}. Линейните модели се оказват неприложими за множество времеви редове от реалната практика \cite{Bontempi-01}. Изкуствените неверонни мрежи се оказват един от подходите да се премине от линейност \cite{Zhang-03} към нелинейност в моделите. Трудностите при изкуствените невронни мрежи са свързани с по-големия брой параметри, които са трудни за определяне, като степен за обучение (learning rate) при обратно разпространение на грешката или размер на скрития слой \cite{Tang-01}. В някои разработки се правят опити да се автоматизира определянето на параметрите \cite{Yan-01}. Липсата на систематизиран подход за изграждане на изкуствени невронни мрежи \cite{Qi-01} е пряко свързан с определянето на параметрите. Линейност и нелинейност могат да се съчетаят в хибридни реализации, базирани на плъзгащи средни и изкуствени невронни мрежи \cite{Zhang-01}. Недостатък при изкуствените неверонни мрежи е нуждата от по-голям брой тренировъчни примери \cite{Lachtermacher-01}. Освен броя на тренировъчните примери, изключително съществен параметър се явява размерът на входния слой, който определя прозореца от стойности в миналия периода от миналото \cite{Chen-01}. С добавяне на обратни връзки в изкуствените невронни мрежи се постига ефект на кратковременна памет. Ефектът на кратковременната памет в изкуствените невронни мрежи често води до подобряване на получените прогнози \cite{Cao-02}. Ускорение в процеса по обучението на изкуствени невронни мрежи може да се получи чрез използването на вероятностни невронни мрежи (Probabilistic Neural Networks) \cite{Khashei-02}. Комбинацията от линейни уравнения за постигането на разграничение при нелинейни класове се постига и чрез машина с поддържащи вектори (Support Vector Machines) \cite{Kyoung-jae-01}. Машините с поддържащи вектори се явяват подобрение на изкуствените невронни мрежи обучавани по правилото за обратно разпространение на грешката. Времето за обучение се намалява, докато достоверността на прогнозите се увеличава \cite{Tay-01}. Ефективността на машините с поддържащи вектори може да се подобри, когато се използват алгоритми за адаптиране на параметрите \cite{Cao-01}. При времеви редове с ясно изразен тренд и сезонност \cite{Zhang-04}, премахването им може да подобри възможностите за генериране на прогноза \cite{Zhang-02}, включително и в комбинация с изкуствени неверонни мрежи \cite{Jain-01}. От оригиналния времеви ред се изваждат стойностите на тренда и подбрани циклични функции \cite{Nelson-01} или уейвлети \cite{Joo-01}. При слабо изразена сезонност, по-удачно може да се окаже сезонността да не се премахва \cite{Hamzacebi-01}. Възможност за генериране на прогнози дават и алгоритмите за разпознаване на образи (Patterns Recognition). Времевият ред може да бъде изследван за наличието на конкретни шаблони/образи и прогнозата да се разпознава на появата на конкретен шаблон \cite{Singh-01}. При много зашумени времеви редове е удачно да се приложи предварителна обработка на данните, така че шумът да се премахне или редуцира \cite{Lu-01}.

\section{Обучение на изкуствени невронни мрежи}

Най-разпространеният вид изкуствени неверонни мрежи е многослойният перцептрон. Структурата им е под формата на насочен тегловен граф, който може да има различни конфигурации \cite{da-Silva-01}. Организацията е на слоеве \cite{Jain-02} (най-често три - входен, скрит и изходен \cite{Pradhan-01}), като най-популярната практика е всички възли между два съседни слоя да са пълно свързани по между си. Пълната свързаност между слоевете понякога се нарушава с цел по-ефективно обучение \cite{Mocanu-01}. Този тип мрежи се използват за решаването на задачи в които са налични тренировъчни примери. Такъв тип обучение е известно като обучение с учител. Целта на изкуствената невронна мрежа е по входно-изходните двойки (тренировъчни примери) да формира функционална зависимост \cite{Kattan-01}. Този механизъм на работа прави многослойните перцептрони идеални за решаване на задачи за класификация \cite{Kubat-01} или прогнозиране \cite{Basheer-01}. Прогнозирането във финансовия свят е задача с голяма сложност при силна нелинейност, зависеща много и различни фактори \cite{Bing-01}. Процесът на самото обучение се състои в намирането на такива стойности за теглата в графа, че изкуствената невронна мрежа максимално добре да онагледява зависимостта между входните данни и изходните данни, без това да доведе до пренапасване (overfitting) \cite{Bilbao-01}. Задачата е оптимизационна и е дефинирана в многомерни нелинейни пространства \cite{Kingston-01}. Най-често началните стойности на теглата се избират случайно в близка околност около нулата, но е възможно първоначалните стойности да се определят с някаква форма на евристика, примерно генетични алгоритми \cite{Chandwani-01}. Теглата на мрежата най-често участват в линейна комбинация с входните сигнали. Получената сума се подлага на нормиране, чрез прилагане на прагова функция (функция на активация) \cite{Ertugrul-01}. Предварителна обработка на данните, преди да бъдат подадени на входа на изкуствената невронна мрежа, често подобряват процеса на обучението \cite{Nawi-01}. Липсващи стойности в множеството от входни данни оказват влияние при процеса на обучението на изкуствената невронна мрежа \cite{Viharos-01}. Организация от три слоя е една от най-често използваните в съчетание с точен числен метод за обучение наречен обратно разпространение на грешката \cite{Pomerleau-01}. Алгоритъмът с обратно разпространение на грешката дава възможности да бъде подобряван по различни начини \cite{Kollias-01}, което най-много се налага заради склонността да попада в локални минимуми \cite{Chen-02}. Освен точни числени методи, при обучението на изкуствени неверонни мрежи приложение намират и стохастичните еволюционни методи \cite{Slowik-01}, които често са вдъхновени от природни феномени \cite{Cui-01}. Точните числени методи превъзхождат евристичните по ефективност на обучението \cite{Piotrowski-01}, но евристичните методи дават повече възможности за избягване на локални оптимуми. Също така, градиентните точни числени методи изискват диференцируеми функции \cite{Karaboga-01}, което спомага в търсенето на глобални оптимуми \cite{Roy-01}. Популационните евристични методи имат много висока степен за паралелна обработка и са изключително подходящи при паралелни изчисления \cite{Ding-01} или разпределени изчисления. Добавянето на случаен шум (примерно нормално разпределен \cite{Zur-01}) може значително да подобри процеса по обучението на многослойни изкуствени невронни мрежи \cite{Sietsma-01}. Случайното изключване на различни възли от скрития слой също може да доведе до подобрено обучение \cite{Sequin-01}. Освен изключването на неврони, възможно е обучението да започва с по-голяма по размер мрежа, а с времето размера да се намалява (алгоритъм за подрязване) \cite{Karnin-01}. Противоположно на намаляването е подхода за избор на мрежова топология чрез нарастване \cite{Yao-01}. В някои ситуации нарастването на размера на изкуствената невронна мрежа води до експоненциално нарастване на нужните за обучението изчислителни ресурси \cite{Wilamowski-01}. Ранно спиране на обучението също може да подобри обобщаващите свойства на изкуствената невронна мрежа \cite{Coulibaly-01}.

\section{Евристични оптимизационни алгоритми}

Двете основни направления в глобалните оптимизационни алгоритми са точните числени методи и евристичните. При точните числени методи най-често се използва пресмятането на градиент и следването му и често са неприложими за нелинейни проблеми \cite{Koziel-01}. При евристичните методи за глобална оптимизация се залага на някаква евристика (интуитивно очакване) за намиране на решения по-близки до глобалните оптимуми, като приложението е в множество области \cite{Slowik-02}, в които точните числени методи не биха дали разумен резултат в приемливо време. Целта е да се постигне глобална сходимост при търсенето на оптимално решение \cite{Beyer-01}, което понякога се постига и с комбинация от евристики \cite{Grosan-01}, като дори евристиките не винаги са достатъчно ефективни \cite{He-01}. Евристичните методи много често се реализират с помощта на генерирани случайни числа, генератори на псевдо-случайни числа (чувствителни по отношение на началната инициализация \cite{Eiben-02}) или хаотични последователности \cite{Caponetto-01}. Докато за определени части от евристиката има теоретична обосновка, то в повечето случаи се използват и параметри за които няма теоретична обосновка, а се избират стойности по подразбиране (примерно, вероятността за кръстосване и вероятността за мутация при генетичните алгоритми) \cite{Eiben-01}. Един от подходите за избор на стойности за параметрите е преди стартирането на изчисленията, а другия подход е адаптивна промяна на стойностите, по време на самото изпълнение. В някои разработки евристични оптимизационни методи се използват за настройка на параметрите при евристична глобална оптимизация. Чрез локално търсене може да се прави фина настройка на параметрите, в самия процес по оптимизация \cite{Karafotias-01}, както и чрез статистически анализ \cite{Francois-01}. Когато евристичните методи се базират на множество от кандидат решения (популация), то те спадат към групата на популационните евристични алгоритми \cite{Whitley-01}. Всеки индивид в популацията представлява вектор от пространството на променливите. Този вектор може да бъде с фиксирана или с плаваща дължина \cite{Ryerkerk-01}. Генетичното разнообразие в популацията може да бъде ключов фактор за ефективен оптимизационен процес \cite{Ursem-01}. При случаите в които наличната популация не съдържа достатъчно разнообразни решения, така че някои решения в пространството на търсенето стават недостижими, може да се приложи схема за въвеждане на изцяло новосъздадена популация \cite{Wegener-01}. Наличието на множество кандидат решения и появата на нови кандидат решения в процеса на оптимизация, често води до забавяне в оценката на решенията, най-вече когато целевата функция се изчислява бавно. При бавни за пресмятане целеви функции е разумно да се търсят някакви начини за ускорена оценка на кандидат решенията \cite{Salami-01}. Едно от най-големите предимства на популационните алгоритми е възможността да се изпълняват паралелно или в разпределена среда \cite{Vikhar-01}. Дори, когато изпълнението се извършва с програмни средства за паралелна обработка, но не еднопроцесорна машина някои популационни алгоритми дават по-добри резултати, спрямо последователната версия на кода \cite{Alba-01}. При оптимизационни проблеми с наложени ограничения (особено нелинейни ограничения) допълнителни мерки трябва да се вземат при генерирането на недопустими решения и разрешаването на такива решения да бъдат част от популацията \cite{Lagaros-01}.

\section{Изчисления в разпределена среда}


